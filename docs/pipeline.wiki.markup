*TABLE OF CONTENTS*
{toc:maxLevel=3}

h1. Introduction

{color:#333333}The Pipeline Jenkins Plugin simplifies building a continuous delivery pipeline with Jenkins by creating a script that defines the steps of your build. For those not familiar with Jenkins Pipeline, please refer to the{color} [Pipeline Tutorial|https://github.com/jenkinsci/pipeline-plugin/blob/master/TUTORIAL.md] {color:#333333}or the{color} [Getting Started With Pipeline|https://jenkins.io/doc/pipeline/] {color:#333333}documentation.{color}

{color:#333333}The{color} [Jenkins Artifactory Plugin|https://github.com/JFrogDev/jenkins-artifactory-plugin] {color:#333333}has been extended to support Artifactory operations as part of the Pipeline script DSL. You have the added option of downloading dependencies, uploading artifacts, and publishing build-info to Artifactory from a Pipeline script.{color}

h1. Using the Artifactory DSL


h2. Creating an Artifactory Server Instance

{color:#333333}To upload or download files to and from your Artifactory server, you need to create an Artifactory server instance in your Pipeline script.{color}

{color:#333333}If your Artifactory server is already defined in Jenkins, you only need its server ID which can be obtained &nbsp;under{color} {color:#333333}{*}Manage \| Configure System.*{color}

{color:#333333}Then, to create your Artifactory server instance, add the following line to your script:{color}

{code}
def server = Artifactory.server 'my-server-id'
{code}
If your Artifactory is not defined in Jenkins you can still create it as follows:
{code}
def server = Artifactory.newServer url: 'artifactory-url', username: 'username', password: 'password'
{code}
You can also user Jenkins Credential ID instead of username and password:
{code}
def server = Artifactory.newServer url: 'artifactory-url', credentialsId: 'ccrreeddeennttiiaall'
{code}
You can modify the server object using the following methods:
{code}
server bypassProxy = true
// If you're using username and password:
server username = 'new-user-name'
server password = 'new-password'
// If you're using Credentials ID:
server credentialsId = 'ccrreeddeennttiiaall'
{code}
{tip:title=Use variables}We recommend using variables rather than plain text to specify the Artifactory server details.{tip}

h2. Uploading and Downloading Files

{color:#333333}To upload or download files you first need to create a spec which is a JSON file that specifies which files should be uploaded or downloaded and the target path.{color}

{color:#333333}For example:{color}

{code}
def downloadSpec = """{
 "files": [
  {
      "pattern": "bazinga-repo/*.zip",
      "target": "bazinga/"
    }
 ]
}"""
{code}
{color:#333333}The above spec specifies that all ZIP files in the{color} {color:#333333}{_}bazinga-repo{_}{color}{color:#333333}&nbsp;Artifactory repository should be downloaded into the{color} {color:#333333}{_}bazinga{_}{color} {color:#333333}directory on your Jenkins agent file system.{color}
{tip:title="files" is an array}{color:#333333}Since the "files" element is an array, you can specify several patterns and corresponding targets in a single download spec.{color}
{tip}

{color:#333333}To download the files, add the following line to your script:{color}

{code}
server.download(downloadSpec)
{code}

{color:#333333}Uploading files is very similar. The following example uploads all ZIP files that include{color} {color:#333333}{_}froggy{_}{color} {color:#333333}in their names into the{color} {color:#333333}{_}froggy-files{_}{color} {color:#333333}folder in the{color} {color:#333333}{_}bazinga-repo{_}{color} {color:#333333}Artifactory repository.{color}

{code}
def uploadSpec = """{
  "files": [
    {
      "pattern": "bazinga/*froggy*.zip",
      "target": "bazinga-repo/froggy-files/"
    }
 ]
}"""
server.upload(uploadSpec)
{code}

h2. Publishing Build-Info to Artifactory

{color:#333333}Both the download and upload methods return a build-info object which can be published to Artifactory as shown in the following examples:{color}

{code}
def buildInfo1 = server.download(downloadSpec)
def buildInfo2 = server.upload(uploadSpec)
buildInfo1.append(buildInfo2)
server.publishBuildInfo(buildInfo1)
{code}

{code}
def buildInfo = Artifactory.newBuildInfo()
server.download(artifactoryDownloadDsl, buildInfo)
server.upload(artifactoryUploadDsl, buildInfo)
server.publishBuildInfo(buildInfo)
{code}

h3. Capturing Environment Variables

{color:#333333}To set the Build-Info object to automatically capture environment variables while downloading and uploading files, add the following to your script:{color}

{code}
def buildInfo = Artifactory.newBuildInfo()
buildInfo.env.capture = true
{code}

By default, environment variables named "password", "secret", or "key" are excluded and will not be published to Artifactory.

You can add more include/exclude patterns as follows:

{code}
def buildInfo = Artifactory.newBuildInfo()
buildInfo.env.filter.addInclude("*a*")
buildInfo.env.filter.addExclude("DONT_COLLECT*")
{code}

Here's how you reset to the include/exclude patterns default values:

{code}
buildInfo.env.filter.reset()
{code}

You can also completely clear the include/exclude patterns:

{code}
buildInfo.env.filter.clear()
{code}

{color:#333333}To collect environment variables at any point in the script, use:{color}

{code}
buildInfo.env.collect()
{code}

{color:#333333}You can get the value of an environment variable collected as follows:{color}

{code}
value = buildInfo.env.vars['env-var-name']
{code}

h3. Triggering Build Retention

{color:#333333}To trigger build retention when publishing build-info to Artifactory, use the following method:{color}

{code}
buildInfo.retention maxBuilds: 10
{code}

{code}
buildInfo.retention maxDays: 7
{code}

{code}
// deleteBuildArtifacts is false by default.

buildInfo.retention maxBuilds: 10, maxDays: 7, doNotDiscardBuilds: ["3", "4"], deleteBuildArtifacts: true
{code}

h2. Promoting Builds in Artifactory

To promote a build between repositories in Artifactory, define the promotion parameters in a promotionConfig object and promote that. For example:

{code}
    def promotionConfig = [
        // Mandatory parameters
        'buildName'          : buildInfo.name,
        'buildNumber'        : buildInfo.number,
        'targetRepo'         : 'libs-release-local',

        // Optional parameters
        'comment'            : 'this is the promotion comment',
        'sourceRepo'         : 'libs-snapshot-local',
        'status'             : 'Released',
        'includeDependencies': true,
        'copy'               : true,
        // 'failFast' is true by default.
        // Set it to false, if you don't want the promotion to abort upon receiving the first error.
        'failFast'           : true
    ]

    // Promote build
    server.promote promotionConfig
{code}

h2. Maven Builds with Artifactory

Maven builds can resolve dependencies, deploy artifacts and publish build-info to Artifactory. To run Maven builds with Artifactory from your Pipeline script, you first need to create an Artifactory server instance, as described at the beginning of this article.
Here's an example:

{code}
    def server = Artifactory.server('my-server-id')
{code}

The next step is to create an Artifactory Maven Build instance:

{code}
    def rtMaven = Artifactory.newMavenBuild()
{code}

Now let's define where the Maven build should download its dependencies from. Let's say you want the release dependencies to be resolved from the 'libs-release' repository and the snapshot dependencies from the 'libs-snapshot' repository. Both repositories are located on the Artifactory server instance you defined above. Here's how you define this, using the Artifactory Maven Build instance we created:

{code}
    rtMaven.resolver server: server, releaseRepo: 'libs-release', snapshotRepo: 'libs-snapshot'
{code}

Now let's define where our build artifacts should be deployed to. Once again, we define the Artifactory server and repositories on the 'rtMaven' instance:

{code}
    rtMaven.deployer server: server, releaseRepo: 'libs-release-local', snapshotRepo: 'libs-snapshot-local'
{code}

By default, all the build artifacts are deployed to Artifactory. In case you want to deploy only some artifacts, you can filter them based on their names, using the 'addInclude' method. In the following example, we are deploying only artifacts with names that start with 'frog'

{code}
    rtMaven.deployer.artifactDeploymentPatterns.addInclude("frog*")
{code}

You can also exclude artifacts from being deployed. In the following example, we are deploying all artifacts, except for those that are zip files:

{code}
    rtMaven.deployer.artifactDeploymentPatterns.addExclude("*.zip")
{code}

And to make things more interesting, you can combine both methods. For example, to deploy all artifacts with names that start with 'frog', but are not zip files, do the following:

{code}
    rtMaven.deployer.artifactDeploymentPatterns.addInclude("frog*").addExclude("*.zip")
{code}

If you'd like to add custom properties to the deployed artifacts, you can do that as follows:

{code}
    rtMaven.deployer.addProperty("status", "in-qa").addProperty("compatibility", "1", "2", "3")
{code}

In some cases, you want to disable artifacts deployment to Artifactory or make the deployment conditional. Here's how you do it:
{code}
    rtMaven.deployer.deployArtifacts = false
{code}

To select a Maven installation for our build, we should define a Maven Tool through Jenkins Manage, and then, set the tool name as follows:

{code}
    rtMaven.tool = 'maven tool name'
{code}

Here's how you define Maven options for your build:

{code}
    rtMaven.opts = '-Xms1024m -Xmx4096m'
{code}

In case you'd like Maven to use a different JDK than your build agent's default, no problem.
Simply set the JAVA_HOME environment variable to the desired JDK path (the path to the directory above the bin directory, which includes the java executable).
Here's you do it:

{code}
    env.JAVA_HOME = 'path to JDK'
{code}

OK, we're ready to run our build. Here's how we define the pom file path (relative to the workspace) and the Maven goals. The deployment to Artifactory is performed during the 'install' phase:

{code}
    def buildInfo = rtMaven.run pom: 'maven-example/pom.xml', goals: 'clean install'
{code}

The above method runs the Maven build. At the beginning of the build, dependencies are resolved from Artifactory and at the end of it, artifacts are deployed to Artifactory.
What about the build information?
The build information has not yet been published to Artifactory, but it is stored locally in the 'buildInfo' instance returned by the 'run' method. You can now publish it to Artifactory as follows:

{code}
    server.publishBuildInfo buildInfo
{code}

You can also merge multiple buildInfo instances into one buildInfo instance and publish it to Artifactory as one build, as described in the 'Publishing Build-Info to Artifactory' section in this article.

h2. Gradle Builds with Artifactory

Gradle builds can resolve dependencies, deploy artifacts and publish build-info to Artifactory. To run Gradle builds with Artifactory from your Pipeline script, you first need to create an Artifactory server instance, as described at the beginning of this article.
Here's an example:

{code}
    def server = Artifactory.server 'my-server-id'
{code}

The next step is to create an Artifactory Gradle Build instance:

{code}
    def rtGradle = Artifactory.newGradleBuild()
{code}

Now let's define where the Gradle build should download its dependencies from. Let's say you want the dependencies to be resolved from the 'libs-release' repository, located on the Artifactory server instance you defined above. Here's how you define this, using the Artifactory Gradle Build instance we created:

{code}
    rtGradle.resolver server: server, repo: 'libs-release'
{code}

Now let's define where our build artifacts should be deployed to. Once again, we define the Artifactory server and repositories on the 'rtGradle' instance:

{code}
    rtGradle.deployer server: server, repo: 'libs-release-local'
{code}

By default, all the build artifacts are deployed to Artifactory. In case you want to deploy only some artifacts, you can filter them based on their names, using the 'addInclude' method. In the following example, we are deploying only artifacts with names that start with 'frog'

{code}
    rtGradle.deployer.artifactDeploymentPatterns.addInclude("frog*")
{code}

You can also exclude artifacts from being deployed. In the following example, we are deploying all artifacts, except for those that are zip files:

{code}
    rtGradle.deployer.artifactDeploymentPatterns.addExclude("*.zip")
{code}

And to make things more interesting, you can combine both methods. For example, to deploy all artifacts with names that start with 'frog', but are not zip files, do the following:

{code}
    rtGradle.deployer.artifactDeploymentPatterns.addInclude("frog*").addExclude("*.zip")
{code}

If you'd like to add custom properties to the deployed artifacts, you can do that as follows:

{code}
    rtGradle.deployer.addProperty("status", "in-qa").addProperty("compatibility", "1", "2", "3")
{code}

In some cases, you want to disable artifacts deployment to Artifactory or make the deployment conditional. Here's how you do it:
{code}
    deployer.deployer.deployArtifacts = deployToArtifactory
{code}

In case the "com.jfrog.artifactory" Gradle Plugin is already applied in your Gradle script, we need to let Jenkins know it shouldn't apply it. Here's how we do it:

{code}
    rtGradle.usesPlugin = true
{code}

To select a Gradle installation for our build, we should define a Gradle Tool through Jenkins Manage, and then, set the tool name as follows:

{code}
    rtMaven.tool = 'gradle tool name'
{code}

In case you'd like Maven to use a different JDK than your build agent's default, no problem.
Simply set the JAVA_HOME environment variable to the desired JDK path (the path to the directory above the bin directory, which includes the java executable).
Here's you do it:

{code}
    env.JAVA_HOME = 'path to JDK'
{code}

OK, looks like we're ready to run our Gradle build. Here's how we define the build.gradle file path (relative to the workspace) and the Gradle tasks. The deployment to Artifactory is performed as part of the 'artifactoryPublish' task:

{code}
    def buildInfo = rtGradle.run rootDir: "projectDir/", buildFile: 'build.gradle', tasks: 'clean artifactoryPublish'
{code}

The above method runs the Gradle build. At the beginning of the build, dependencies are resolved from Artifactory and at the end of it, artifacts are deployed to Artifactory.
What about the build information?
The build information has not yet been published to Artifactory, but it is stored locally in the 'buildInfo' instance returned by the 'run' method. You can now publish it to Artifactory as follows:

{code}
    server.publishBuildInfo buildInfo
{code}

You can also merge multiple buildInfo instances into one buildInfo instance and publish it to Artifactory as one build, as described in the 'Publishing Build-Info to Artifactory' section in this article.

That's it\! We're all set.

The rtGradle instance supports additional configuration APIs. You can use these APIs as follows:
{code}
    def rtGradle = Artifactory.newGradleBuild()
    // Deploy Maven descriptors to Artifactory:
    rtGradle.deployer.deployMavenDescriptors = true
    // Deploy Ivy descriptors (pom.xml files) to Artifactory:
    rtGradle.deployer.deployIvyDescriptors = true

    // The following properties are used for Ivy publication configuration.
    // The values below are the defaults.

    // Set the deployed Ivy descriptor pattern:
    rtGradle.deployer.ivyPattern = '[organisation]/[module]/ivy-[revision].xml'
    // Set the deployed Ivy artifacts pattern:
    rtGradle.deployer.artifactPattern = '[organisation]/[module]/[revision]/[artifact]-[revision](-[classifier]'
    // Set mavenCompatible to true, if you wish to replace dots with slashes in the Ivy layout path, to match the Maven layout:
    rtGradle.deployer.mavenCompatible = true
{code}

h2. Maven Release Management with Artifactory

With the Artifactory Pipeline DSL you can easily manage and run a release build for your Maven project. Here's how you do it:

Let's start by cloning the code from our source control:
{code}
    git url: 'https://github.com/eyalbe4/project-examples.git'
{code}

If we built the code right now with git, Maven will create snapshot artifacts, because the pom files include a snapshot version (for example, 1.0.0-SNAPSHOT).
Since we want our build to create release artifacts, let's change the version in the pom file to 1.0.0.
To do that, let's create a mavenDescriptor instance, and set the version to 1.0.0:

{code}
    def descriptor = Artifactory.mavenDescriptor()
    descriptor.version = '1.0.0'
{code}

In case the project's pom file is not located at the root of the cloned project, but inside a sub-directory, let's add it to the mavenDescriptor instance:

{code}
    descriptor.pomFile = 'maven-example/pom.xml'
{code}

In most cases, we'd like to verify that our release build does not include snapshot dependencies. The are two ways to do that. The first one, is to configure the descriptor to fail the build if snapshot dependencies are found in the pom files. The job in this case will fail before the new version is set to the pom files.
Here's how you configure this:

{code}
    descriptor.failOnSnapshot = true
{code}

The second way to verify this is by using the hasSnapshots method, which returns a boolean true value in case snapshot dependencies are found:

{code}
    def snapshots = descriptor.hasSnapshots()
    if (snapshots) {
        ....
    }
{code}

That's it. Using the mavenDescriptor as it is now, will change the version inside the root pom file. In addition, if the project includes sub-modules with pom files, which include a version, it will change them as well.
Sometimes however, some sub-modules should use different release versions. For example, suppose there's one module which version should change to 1.0.1, instead of 1.0.0. The other modules should still have their versions changed to 1.0.0. Here's how we do it:

{code}
    descriptor.setVersion "the.group.id:the.artifact.id", "1.0.1"
{code}

The above setVersion method receives two arguments: the module name and its new release version. The module name is composed of the group ID and the artifact ID with a colon between them.
All right. Let's transform the pom files to include the new versions:

{code}
    descriptor.transform()
{code}

OK. The transform method changed the versions on the local pom files.
You can now build the code and deploy the release Maven artifacts to Artifactory as described in the "Maven Builds with Artifactory" section in this article.

The next step be commuting the changes made to the pom files to the source control, and also tagging the new release version in the source control repository. If you're using git, you can use the git client installed on your build agent and run a few shell commands from inside Pipeline script to do that.
The last thing you'll probably want to do is to change the pom files version to the next development version and commit the changes. You can do that again by using a mavenDescriptor instance.

h2. Docker Builds with Artifactory

The Jenkins Artifactory Plugin supports a Pipeline DSL that enables you to collect and publish build-info to Artifactory for your Docker builds. To collect the build-info, the plugin uses an internal HTTP proxy server, which captures the traffic between the Docker Daemon and your Artifactory reverse proxy. To setup your Jenkins build agents to collect build-info for your Docker builds, please refer to the [setup instructions|Jenkins Artifactory Plugin - Setting Up Docker Build Info]

{code}
    // Create an Artifactory server instance, as described above in this article:
    def server = Artifactory.server 'my-server-id'

    // Create an Artifactory Docker instance. The instance stores the Artifactory credentials and the Docker daemon host address:
    def rtDocker = Artifactory.docker username: 'artifactory-username', password: 'artifactory-password', host: "tcp://<daemon IP>:<daemon port>"

    // If the docker daemon host is not specified, "/var/run/dokcer.sock" is used as a default value:
    def rtDocker = Artifactory.docker username: 'artifactory-username', password: 'artifactory-password'

    // You can also use the Jenkins credentials plugin instead of username and password:
    def rtDocker = Artifactory.docker credentialsId: 'ccrreeddeennttiiaall'

    // Push a docker image to Artifactory (here we're pushing hello-world:latest). The push method also expects
    // Artifactory repository name:
    def buildInfo = rtDocker.push('<artifactory-docker-registry-url>/hello-world:latest', '<target-artifactory-repository>')

    // Publish the build-info to Artifactory:
    server.publishBuildInfo buildInfo
{code}

h2. Scanning Builds with JFrog Xray

From version 2.9.0, Jenkins Artifactory Plugin is integrated with JFrog Xray through JFrog Artifactory allowing you to have build artifacts and dependencies scanned for vulnerabilities and other issues. If issues or vulnerabilities are found, you may choose to fail a build job or perform other actions according to the Pipeline script you write.&nbsp;This integration requires *JFrog Artifactory v4.16* and above and *JFrog Xray v1.6* and above.&nbsp;

You may scan any build that has been published to Artifactory. It does not matter when the build was published, as long as it was published before triggering the scan by JFrog Xray.

The following instructions show you how to configure your Pipeline script to have a build scanned.

First, create a scanConfig instance with the build name and build number you wish to scan:

{code}
  def scanConfig = [
      'buildName'     : 'my-build-name',
      'buildNumber'   : '17'
    ]
{code}

If you're scanning a build which has already been published to Artifactory in the same job, you can use the build name and build number stored on the buildInfo instance you used to publish the build. For example:

{code}
  server.publishBuildInfo buildInfo
  def scanConfig = [
      'buildName'      : buildInfo.name,
      'buildNumber'    : buildInfo.number
    ]
{code}

Before you trigger the scan, there's one more thing you need to be aware of. By default, if the Xray scan finds vulnerabilities or issues in the build that trigger an alert, the build job will fail. If you don't want the build job to fail, you can add the 'failBuild' property to the scanConfig instance and set it to 'false' as shown here:

{code}
  def scanConfig = [
      'buildName'      : buildInfo.name,
      'buildNumber'    : buildInfo.number,
      'failBuild'      : false
    ]
{code}

OK, we're ready to initiate the scan. The scan should be initiated on the same Artifactory server instance, to which the build was published:

{code}
 def scanResult = server.xrayScan scanConfig
{code}

That's it. The build will now be scanned. If the scan is not configured to fail the build job, you can use the scanResult instance returned from the xrayScan method to see some details about the scan.
For example, to print the result to the log, you could use the following code snippet:

{code}
 echo xrayResults as String
{code}

For more details on the integration with JFrog Xray and JFrog Artifactory to scan builds for issues and vulnerabilities, please refer to [CI/CD Integration|https://www.jfrog.com/confluence/display/XRAY/CI-CD+Integration] in the JFrog Xray documentation.

h1. Advanced File Spec Options


h2. Using Placeholders

For additional flexibility in defining your Upload and Download Specs, you have the option of using Placeholders.
You can read about using Placeholders in File Specs [here|https://www.jfrog.com/confluence/display/RTF/Using+File+Specs].

h2. Using Regular Expressions in Upload Specs

Instead of using wildcard patterns, you can use regular expressions in Upload Specs. You just need to add _"regexp"__&nbsp;: "true"_ to the spec as in the following example:

{code}
def uploadSpec = """{
  "files": [
    {
      "pattern": "(.*).tgz",
      "target": "my-local-repo/{1}/"
      "recursive": "false"
      "regexp": "true"
    }
 ]
}"""
{code}

The usage of regular expressions is not supported in Download Specs.

h1. File Spec Schema

You can read the File Spec schema [here|https://www.jfrog.com/confluence/display/RTF/Using+File+Specs].


h2. Examples

The [Jenkins Pipeline Examples|https://github.com/JFrogDev/project-examples/tree/master/jenkins-pipeline-examples]&nbsp;can help get you started using the Artifactory DSL in your Pipeline scripts.